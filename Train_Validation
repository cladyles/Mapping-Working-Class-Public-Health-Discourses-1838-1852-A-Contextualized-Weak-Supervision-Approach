# =================================================================
#     ç¬¬äºŒé˜¶æ®µ_æ¨¡å‹è®­ç»ƒä¸è¿­ä»£æ‰©å±• ä¸‰åˆ†ç±»+å«ç”Ÿæƒé‡1.5
# =================================================================

# ----------------- ç¯å¢ƒé…ç½®å’Œå¯¼å…¥ -----------------
import os
import sys
import random
import tensorflow as tf
import pickle
import json
import gc
import math
import numpy as np
from datetime import datetime
import nltk
import csv
from sklearn.utils.class_weight import compute_class_weight

# å¼ºåˆ¶ç¦ç”¨XLAä»¥è§£å†³å…¼å®¹æ€§é—®é¢˜
os.environ["TF_XLA_FLAGS"] = "--tf_xla_enable_xla_devices=false"
os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'

# ç¡®ä¿Pythonèƒ½å¤Ÿæ‰¾åˆ°ä½ çš„é¡¹ç›®æ–‡ä»¶å¤¹
try:
    sys.path.append('/content/drive/MyDrive/Dissertation/allinone/ConWea/')
    from util import *
except FileNotFoundError:
    print(" ERROR: Project path not found or util.py is missing.")
    pass

# =================================================================
#             --- è„šæœ¬æ€»å¼€å…³ ---
# =================================================================
# åœ¨è¿™é‡Œæ§åˆ¶æ˜¯å¦è¿›è¡Œå¾®è°ƒã€‚True = è¿›è¡Œå¾®è°ƒ, False = è·³è¿‡å¾®è°ƒ
DO_FINETUNE = True
DO_SAVE_MODEL = True
INTERACTIVE_PRUNING = True # <--- è®¾ç½®ä¸º True æ¥å¯ç”¨æ‰‹åŠ¨è£å‰ªåŠŸèƒ½
# =================================================================

# å›ºå®šéšæœºç§å­ï¼Œæå‡å¤ç°æ€§
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# NLTKèµ„æºä¸‹è½½
print("Downloading NLTK resources...")
try:
    nltk.download('punkt_tab', quiet=True)
    nltk.download('stopwords', quiet=True)
    print("NLTK resources downloaded.")
except Exception as _e:
    print("NLTK download failed:", _e)

# ä¾èµ–åº“
from sklearn.metrics import classification_report, f1_score, accuracy_score
from keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.feature_extraction.text import CountVectorizer
from collections import defaultdict
from gensim.models import word2vec
from nltk.corpus import stopwords
import keras
from keras.layers import (
    Dense, GRU, TimeDistributed, Input,
    Embedding, Bidirectional, Lambda,
    Masking
)
from keras.models import Model
from keras import backend as K

# ---- Mixed Precision ----
try:
    try:
        from tensorflow.keras import mixed_precision as mp
    except ImportError:
        import keras.mixed_precision as mp
    mp.set_global_policy('mixed_float16')
    print("[MP] mixed_float16 enabled.")
except Exception as _e:
    print("[MP] mixed precision not enabled:", _e)

# ---- æ‰¹é‡å¤§å°å€™é€‰ + å­¦ä¹ ç‡ç¼©æ”¾ ----
BATCH_CANDIDATES = [1024, 512, 256]
BASE_LR = 1e-3

def lr_for_batch(bs, base=BASE_LR):
    scale = {256: 1.0, 512: 1.5, 1024: 2.0}.get(bs, bs / 256)
    return base * scale

# ----------------- æ—¥å¿—ä¸å¿«ç…§å·¥å…· -----------------
def ensure_dir(p):
    os.makedirs(p, exist_ok=True)

def save_json(obj, path):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

def append_metrics_csv(csv_path, row_dict, header_order):
    header_needed = not os.path.exists(csv_path)
    with open(csv_path, "a", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=header_order)
        if header_needed:
            w.writeheader()
        w.writerow(row_dict)

def components_topN(components, N=20):
    out = {}
    for lbl, d in components.items():
        pairs = sorted(d.items(), key=lambda kv: kv[1].get("rank", 0.0), reverse=True)[:N]
        out[lbl] = [{"word": w, **stats} for w, stats in pairs]
    return out

# =================================================================
#     å†…ç½®æ¨¡å‹å®šä¹‰ (ä¸å†ä¾èµ–å¤–éƒ¨ .py æ–‡ä»¶)
# =================================================================
class AttentionLayer(keras.layers.Layer):
    def __init__(self, context_vector_length=100, **kwargs):
        super().__init__(**kwargs)
        self.context_vector_length = context_vector_length
    def build(self, input_shape):
        dim = input_shape[2]
        self.W = self.add_weight(name='W', shape=(dim, self.context_vector_length), initializer='uniform', trainable=True)
        self.u = self.add_weight(name='context_vector', shape=(self.context_vector_length, 1), initializer='uniform', trainable=True)
        super().build(input_shape)
    def _get_attention_weights(self, X):
        u_tw = keras.activations.tanh(tf.linalg.matmul(X, self.W))
        tw_stimulus = tf.linalg.matmul(u_tw, self.u)
        tw_stimulus = tf.squeeze(tw_stimulus, axis=-1)
        return keras.activations.softmax(tw_stimulus, axis=-1)
    def call(self, X):
        att_weights = self._get_attention_weights(X)
        att_weights_reshaped = tf.expand_dims(att_weights, axis=-1)
        weighted_input = X * att_weights_reshaped
        return tf.reduce_sum(weighted_input, axis=1)
    def get_config(self):
        config = super().get_config()
        config.update({'context_vector_length': self.context_vector_length})
        return config

class HAN(Model):
    def __init__(
            self, max_words, max_sentences, output_size,
            embedding_matrix, word_encoding_dim=200,
            sentence_encoding_dim=200, name='han-for-docla', **kwargs
    ):
        super().__init__(name=name, **kwargs)
        self.max_words = max_words
        self.max_sentences = max_sentences
        self.output_size = output_size
        self.embedding_matrix = np.array(embedding_matrix)
        self.word_encoding_dim = word_encoding_dim
        self.sentence_encoding_dim = sentence_encoding_dim
        self.word_encoder_model = self._build_word_encoder()
        self.word_attention = TimeDistributed(AttentionLayer(), name='word_attention')
        self.sentence_encoder_model = self._build_sentence_encoder()
        self.sentence_attention = AttentionLayer(name='sentence_attention')
        self.class_prediction = Dense(self.output_size, activation='softmax', name='class_prediction', dtype='float32')
    def _build_word_encoder(self):
        assert self.word_encoding_dim % 2 == 0
        vocab_size, embed_dim = self.embedding_matrix.shape
        embedding_layer = Embedding(
            vocab_size, embed_dim, weights=[self.embedding_matrix],
            input_length=self.max_words, trainable=False, mask_zero=True
        )
        sentence_input = Input(shape=(self.max_words,), dtype='int32')
        embedded = embedding_layer(sentence_input)
        encoded = Bidirectional(GRU(int(self.word_encoding_dim / 2), return_sequences=True, recurrent_dropout=0.1))(embedded)
        return Model(inputs=sentence_input, outputs=encoded, name='word_encoder')
    def _build_sentence_encoder(self):
        assert self.sentence_encoding_dim % 2 == 0
        text_input = Input(shape=(self.max_sentences, self.word_encoding_dim))
        encoded = Bidirectional(GRU(int(self.sentence_encoding_dim / 2), return_sequences=True, recurrent_dropout=0.1))(text_input)
        return Model(inputs=text_input, outputs=encoded, name='sentence_encoder')
    def call(self, inputs):
        embedded_sentences = TimeDistributed(self.word_encoder_model)(inputs)
        word_att_output = self.word_attention(embedded_sentences)
        doc_encoded = self.sentence_encoder_model(word_att_output)
        doc_summary = self.sentence_attention(doc_encoded)
        return self.class_prediction(doc_summary)
    def get_config(self):
        config = super().get_config()
        config.update({
            'max_words': self.max_words, 'max_sentences': self.max_sentences,
            'output_size': self.output_size, 'embedding_shape': self.embedding_matrix.shape,
            'word_encoding_dim': self.word_encoding_dim, 'sentence_encoding_dim': self.sentence_encoding_dim
        })
        return config
    @classmethod
    def from_config(cls, config):
        embedding_shape = config.pop('embedding_shape')
        dummy_matrix = np.zeros(embedding_shape)
        return cls(embedding_matrix=dummy_matrix, **config)

# =================================================================
#     æ‰€æœ‰å¿…éœ€çš„è¾…åŠ©å‡½æ•°
# =================================================================
def train_word2vec(df, dataset_path):
    def get_embeddings(inp_data, tokenizer, size_features=100, mode='skipgram', min_count=1, context=5):
        num_workers = 15
        downsampling = 1e-3
        print('Training Word2Vec model...')
        vocabulary_inv = {v: k for k, v in tokenizer.word_index.items()}
        sentences = [[vocabulary_inv[w] for w in s if w in vocabulary_inv] for s in inp_data]
        sentences = [s for s in sentences if s]
        if not sentences:
            print("Warning: No valid sentences in corpus for Word2Vec training.")
            return np.zeros((len(vocabulary_inv) + 1, size_features))
        sg = 1 if mode == 'skipgram' else 0
        print('Model:', 'skip-gram' if sg == 1 else 'CBOW')
        embedding_model = word2vec.Word2Vec(
            sentences=sentences, workers=num_workers, sg=sg,
            vector_size=size_features, min_count=min_count,
            window=context, sample=downsampling
        )
        embedding_model.train(
            corpus_iterable=sentences,
            total_examples=embedding_model.corpus_count,
            epochs=embedding_model.epochs
        )
        embedding_weights = np.zeros((len(vocabulary_inv) + 1, size_features))
        embedding_weights[0] = 0
        for i, word in vocabulary_inv.items():
            if word in embedding_model.wv:
                embedding_weights[i] = embedding_model.wv[word]
            else:
                embedding_weights[i] = np.random.uniform(-0.25, 0.25, embedding_model.vector_size)
        return embedding_weights

    tokenizer = fit_get_tokenizer(df.sentence, max_words=150000)
    print("Total number of words: ", len(tokenizer.word_index))
    tagged_data = tokenizer.texts_to_sequences(df.sentence)
    embedding_mat = get_embeddings(tagged_data, tokenizer)
    pickle.dump(tokenizer, open(os.path.join(dataset_path, "tokenizer8.pkl"), "wb"))
    pickle.dump(embedding_mat, open(os.path.join(dataset_path, "embedding_matrix8.pkl"), "wb"))

def preprocess(df, word_cluster):
    print("Preprocessing data..")
    stop_words = set(stopwords.words('english'))
    stop_words.add('would')
    word_vec = {}
    for index, row in df.iterrows():
        if index % 1000 == 0:
            print(f"Finished rows: {index} out of {str(len(df))}")
        line = row["sentence"]
        words = line.strip().split()
        new_words = []
        for word in words:
            if word not in word_vec:
                vec = get_vec(word, word_cluster, stop_words)
                if len(vec) == 0:
                    continue
                word_vec[word] = vec
            new_words.append(word)
        df.loc[index, "sentence"] = " ".join(new_words)
    return df, word_vec

def generate_pseudo_labels(df, labels, label_term_dict, tokenizer, label_weight_map):
    """
    åŸºäºç§å­è¯ç”Ÿæˆä¼ªæ ‡ç­¾ï¼ˆåŠ æƒæŠ•ç¥¨ï¼‰ï¼š
      - [æ–°] é‡‡ç”¨å½’ä¸€åŒ–åˆ†æ•°ï¼Œæ¶ˆé™¤ç±»åˆ«å¤§å°åè§
      - æ¯ä¸ªæ ‡ç­¾çš„æœ€ç»ˆåˆ†æ•° = (å‘½ä¸­ç§å­è¯æ•° / è¯¥æ ‡ç­¾æ€»ç§å­è¯æ•°) * ç±»åˆ«æƒé‡
      - å«ç”Ÿç±»æ ‡ç­¾æƒé‡1.5ï¼Œå…¶ä»–1.0ï¼ˆç”± label_weight_map æä¾›ï¼‰
    """
    y, X, y_true = [], [], []
    index_word = {v: k for k, v in tokenizer.word_index.items()}

    print("Generating pseudo-labels with NORMALIZED scoring...") # æ·»åŠ æ—¥å¿—ï¼Œç¡®è®¤æ–°ç‰ˆå‡½æ•°è¢«è°ƒç”¨

    for index, row in df.iterrows():
        if index > 0 and index % 20000 == 0:
             print(f"  > Processing row {index}/{len(df)}") # æ·»åŠ å¤„ç†è¿›åº¦

        line, label = row["sentence"], row["label"]
        tokens = tokenizer.texts_to_sequences([line])[0]
        words = [index_word[tok] for tok in tokens if tok in index_word]

        score_dict = {}  # {label: float_score}
        has_any = False

        for l in labels:
            seed_words = set(label_term_dict.get(l, []))

            # --- æ ¸å¿ƒä¿®æ”¹å¼€å§‹ ---

            # 1. è·å–ç±»åˆ«æ€»ç§å­è¯æ•°ï¼Œå¹¶é˜²æ­¢é™¤ä»¥0çš„é”™è¯¯
            total_seeds = len(seed_words)
            if total_seeds == 0:
                continue

            # 2. è®¡ç®—å¥å­ä¸­çš„è¯å‘½ä¸­è¯¥ç±»åˆ«ç§å­è¯çš„æ¬¡æ•°
            #    æ³¨æ„: è¿™é‡Œéµå¾ªä½ åŸå§‹é€»è¾‘ï¼Œé‡å¤å‘½ä¸­ä¼šé‡å¤è®¡æ•°ã€‚
            #    å¦‚æœæƒ³æŒ‰å»é‡è¯è®¡æ•°ï¼Œå¯ä»¥æŠŠ `words` æ›¿æ¢ä¸º `set(words)`
            hit_count = sum(1.0 for wd in words if wd in seed_words)

            if hit_count == 0:
                continue

            has_any = True

            # 3. è®¡ç®—å½’ä¸€åŒ–åˆ†æ•°ï¼ˆå³â€œå‘½ä¸­ç‡â€ï¼‰ï¼Œè¿™æ¶ˆé™¤äº†å¤§ç±»åˆ«å¤©ç”Ÿå ä¼˜çš„é—®é¢˜
            normalized_score = hit_count / total_seeds

            # 4. è·å–è¯¥ç±»åˆ«çš„ç‰¹å®šæƒé‡ï¼ˆä¾‹å¦‚ health çš„ 1.5ï¼‰
            w = float(label_weight_map.get(l, 1.0))

            # 5. è®¡ç®—æœ€ç»ˆåŠ æƒåˆ†æ•°
            final_score = normalized_score * w

            # --- æ ¸å¿ƒä¿®æ”¹ç»“æŸ ---

            score_dict[l] = score_dict.get(l, 0.0) + final_score

        if has_any:
            # ä½¿ç”¨ lambda å‡½æ•°å®‰å…¨åœ°æ‰¾å‡ºæœ€é«˜åˆ†æ ‡ç­¾ï¼Œé¿å… score_dict ä¸ºç©ºæ—¶å‡ºé”™
            best_label = max(score_dict.items(), key=lambda item: item[1])[0] if score_dict else None

            if best_label:
                y.append(best_label)
                X.append(line)
                y_true.append(label)

    # æ‰“å°å‡ºå„ç±»åˆ«çš„æ ·æœ¬æ•°é‡ï¼Œæ–¹ä¾¿ä½ ç«‹åˆ»æ£€æŸ¥æ•ˆæœ
    print("--- Pseudo-label Distribution ---")
    from collections import Counter
    label_counts = Counter(y)
    for lbl, count in label_counts.most_common():
        print(f"  {lbl}: {count} samples")
    print("---------------------------------")


    return X, y, y_true

# æ³¨æ„ï¼šargmax_label å‡½æ•°ä¸å†éœ€è¦ï¼Œå› ä¸ºæˆ‘ä»¬ç›´æ¥åœ¨ä¸Šé¢ç”¨ max() å®ç°äº†
# ä½ å¯ä»¥åˆ é™¤ def argmax_label(...) è¿™ä¸ªè¾…åŠ©å‡½æ•°

def _evaluate_on_val(model, X_val, y_val, index_to_label):
    pred_val = model.predict(X_val, verbose=0)
    y_pred_lbl = get_from_one_hot(pred_val, index_to_label)
    y_true_lbl = get_from_one_hot(y_val, index_to_label)
    acc = accuracy_score(y_true_lbl, y_pred_lbl)
    f1_macro = f1_score(y_true_lbl, y_pred_lbl, average='macro', zero_division=0)
    f1_micro = f1_score(y_true_lbl, y_pred_lbl, average='micro', zero_division=0)
    f1_weighted = f1_score(y_true_lbl, y_pred_lbl, average='weighted', zero_division=0)
    return acc, f1_macro, f1_micro, f1_weighted, y_pred_lbl

def expand_seeds(df, label_term_dict, pred_labels, label_to_index, index_to_label, word_to_index, index_to_word, inv_docfreq, docfreq, it, n1, doc_freq_thresh=5):
    def get_rank_matrix(
        docfreq, inv_docfreq, label_count, label_docs_dict, label_to_index,
        term_count, word_to_index, doc_freq_thresh
    ):
        E_LT = np.zeros((label_count, term_count))
        components = {}
        for l in label_docs_dict:
            components[l] = {}
            docs = label_docs_dict[l]
            if not docs:
                continue
            docfreq_local = calculate_doc_freq(docs)
            vect = CountVectorizer(vocabulary=list(word_to_index.keys()), tokenizer=lambda x: x.split())
            X = vect.fit_transform(docs)
            X_arr = X.toarray()
            rel_freq = np.sum(X_arr, axis=0) / len(docs)
            names = vect.get_feature_names_out()
            for i, name in enumerate(names):
                if name not in docfreq_local or docfreq_local[name] < doc_freq_thresh \
                   or name not in docfreq or name not in inv_docfreq:
                    continue
                rank = (docfreq_local[name] / docfreq[name]) * inv_docfreq[name] * np.tanh(rel_freq[i])
                E_LT[label_to_index[l]][word_to_index[name]] = rank
                components[l][name] = {
                    "reldocfreq": docfreq_local[name] / docfreq[name],
                    "idf": inv_docfreq[name],
                    "rel_freq": np.tanh(rel_freq[i]),
                    "rank": rank
                }
        return E_LT, components

    def disambiguate(label_term_dict, components):
        new_dic = {}
        for l in label_term_dict:
            all_interp_seeds = label_term_dict[l]
            seed_to_all_interp = defaultdict(set)
            disambiguated_seed_list = []
            for item in all_interp_seeds:
                if isinstance(item, (tuple, list)) and len(item) > 0:
                    word = str(item[0])
                else:
                    word = str(item)
                temp = word.split("$")
                if len(temp) == 1:
                    disambiguated_seed_list.append(word)
                else:
                    seed_to_all_interp[temp[0]].add(word)
            for seed, interpretations in seed_to_all_interp.items():
                max_interp, maxi = "", -1
                for interp in interpretations:
                    if l in components and interp in components[l] and components[l][interp]["rank"] > maxi:
                        max_interp, maxi = interp, components[l][interp]["rank"]
                if max_interp:
                    disambiguated_seed_list.append(max_interp)
            new_dic[l] = disambiguated_seed_list
        return new_dic

    def expand(E_LT, index_to_label, index_to_word, it, label_count, n1, old_label_term_dict, label_docs_dict):
        word_map = {}
        zero_docs_labels = set()
        for l in range(label_count):
            label_name = index_to_label[l]
            if not np.any(E_LT[l]) or not label_docs_dict.get(label_name):
                zero_docs_labels.add(label_name)
                continue
            n = min(n1 * (it), int(math.log(len(label_docs_dict[label_name]) + 1, 1.5)))
            inds_popular = E_LT[l].argsort()[::-1][:int(n)]
            for word_ind in inds_popular:
                word = index_to_word[word_ind]
                if word not in word_map or E_LT[l][word_ind] > word_map[word][1]:
                    word_map[word] = (label_name, E_LT[l][word_ind])
        new_label_term_dict = defaultdict(set)
        for word, (label, val) in word_map.items():
            new_label_term_dict[label].add(word)
        for l in zero_docs_labels:
            new_label_term_dict[l] = set(old_label_term_dict.get(l, []))
        return new_label_term_dict

    label_count, term_count = len(label_to_index), len(word_to_index)
    label_docs_dict = get_label_docs_dict(df, label_term_dict, pred_labels)
    E_LT, components = get_rank_matrix(
        docfreq, inv_docfreq, label_count, label_docs_dict, label_to_index,
        term_count, word_to_index, doc_freq_thresh
    )
    if it == 0:
        print("Disambiguating seeds..")
        label_term_dict = disambiguate(label_term_dict, components)
    else:
        print("Expanding seeds..")
        label_term_dict = expand(
            E_LT, index_to_label, index_to_word, it, label_count, n1, label_term_dict, label_docs_dict
        )

    # âœ…âœ…âœ… FIX: Ensure this return statement is correctly indented
    # It should be at the same level as the 'if it == 0:' block above.
    return label_term_dict, components

def disambiguate(label_term_dict, components):
    new_dic = {}
    for l in label_term_dict:
        all_interp_seeds = label_term_dict[l]
        seed_to_all_interp = defaultdict(set)
        disambiguated_seed_list = []

        for item in all_interp_seeds: # Use 'item' to avoid confusion

            # --- START OF FIX ---
            # Check if the item is a tuple or list, and extract the word string
            # This makes the function robust to mixed data types in the seed list.
            if isinstance(item, (tuple, list)) and len(item) > 0:
                word = str(item[0])
            else:
                word = str(item)
            # --- END OF FIX ---

            # Now, the 'word' variable is guaranteed to be a string
            temp = word.split("$")
            if len(temp) == 1:
                disambiguated_seed_list.append(word)
            else:
                seed_to_all_interp[temp[0]].add(word)

        for seed, interpretations in seed_to_all_interp.items():
            max_interp, maxi = "", -1
            for interp in interpretations:
                if l in components and interp in components[l] and components[l][interp]["rank"] > maxi:
                    max_interp, maxi = interp, components[l][interp]["rank"]
            if max_interp:
                disambiguated_seed_list.append(max_interp)
        new_dic[l] = disambiguated_seed_list
    return new_dic

    def expand(E_LT, index_to_label, index_to_word, it, label_count, n1, old_label_term_dict, label_docs_dict):
        word_map = {}
        zero_docs_labels = set()
        for l in range(label_count):
            label_name = index_to_label[l]
            if not np.any(E_LT[l]) or not label_docs_dict.get(label_name):
                zero_docs_labels.add(label_name)
                continue
            n = min(n1 * (it), int(math.log(len(label_docs_dict[label_name]) + 1, 1.5)))
            inds_popular = E_LT[l].argsort()[::-1][:int(n)]
            for word_ind in inds_popular:
                word = index_to_word[word_ind]
                if word not in word_map or E_LT[l][word_ind] > word_map[word][1]:
                    word_map[word] = (label_name, E_LT[l][word_ind])
        new_label_term_dict = defaultdict(set)
        for word, (label, val) in word_map.items():
            new_label_term_dict[label].add(word)
        for l in zero_docs_labels:
            new_label_term_dict[l] = set(old_label_term_dict.get(l, []))
        return new_label_term_dict

    label_count, term_count = len(label_to_index), len(word_to_index)
    label_docs_dict = get_label_docs_dict(df, label_term_dict, pred_labels)
    E_LT, components = get_rank_matrix(
        docfreq, inv_docfreq, label_count, label_docs_dict, label_to_index,
        term_count, word_to_index, doc_freq_thresh
    )
    if it == 0:
        print("Disambiguating seeds..")
        label_term_dict = disambiguate(label_term_dict, components)
    else:
        print("Expanding seeds..")
        label_term_dict = expand(
            E_LT, index_to_label, index_to_word, it, label_count, n1, label_term_dict, label_docs_dict
        )
    return label_term_dict, components

# ========= NEW: ä» label_term_dict è‡ªåŠ¨æ„å»ºæ ‡ç­¾æƒé‡è¡¨ =========
def build_label_weight_map_from_json(label_term_dict):
    """
    æ ¹æ®æ ‡ç­¾åè‡ªåŠ¨èµ‹æƒï¼š
      - å«ç”Ÿç›¸å…³ï¼ˆhealth/hygiene/infection/sanitation/housing/food/workplace/factoryï¼‰â†’ 1.5
      - æ”¿æ²»ç›¸å…³ï¼ˆpolitic/union/strike/parliament/charter/reformï¼‰â†’ 1.0
      - å…¶ä»– â†’ 1.0
    """
    weight_map = {}
    for lbl in label_term_dict.keys():
        ll = lbl.lower()
        if any(k in ll for k in ['health','hygiene','infection','sanitation','housing','food','workplace','factory']):
            weight_map[lbl] = 1.5
        elif any(k in ll for k in ['politic','union','strike','parliament','charter','reform']):
            weight_map[lbl] = 1.0
        else:
            weight_map[lbl] = 1.0
    return weight_map

def train_classifier(iter_idx, df, labels, label_term_dict, label_to_index, index_to_label, dataset_path, logs_dir, do_finetune=False):
    print("Going to train classifier..")
    basepath = dataset_path
    dump_dir = os.path.join(basepath, "models", "conwea")
    tmp_dir = os.path.join(dump_dir, "checkpoints")
    ensure_dir(dump_dir); ensure_dir(tmp_dir); ensure_dir(logs_dir)

    max_sentence_length, max_sentences = 100, 15
    tokenizer = pickle.load(open(os.path.join(dataset_path, "tokenizer8.pkl"), "rb"))
    label_weight_map = build_label_weight_map_from_json(label_term_dict)
    X, y, y_true = generate_pseudo_labels(
        df, labels, label_term_dict, tokenizer, label_weight_map
    )

    y_one_hot = make_one_hot(y, label_to_index)

    # ---- è®¡ç®— class_weight ----
    y_integers = [label_to_index[label] for label in y]
    classes = np.arange(len(labels))
    class_weights_arr = compute_class_weight(
        class_weight="balanced",
        classes=classes,
        y=y_integers
    )
    class_weights = dict(zip(classes, class_weights_arr))
    print("Successfully computed Class Weights:", class_weights)

    print("Splitting into train, dev...")
    X_train, y_train, X_val, y_val = create_train_dev(
        X, labels=y_one_hot, tokenizer=tokenizer,
        max_sentences=max_sentences, max_sentence_length=max_sentence_length
    )

    print("Loading Embedding matrix...")
    embedding_matrix = pickle.load(open(os.path.join(dataset_path, "embedding_matrix8.pkl"), "rb"))

    trained = False
    used_bs, used_lr = None, None
    model = None

    for bs in BATCH_CANDIDATES:
        try:
            K.clear_session(); gc.collect()

            print(f"\n[ATTEMPT] Trying batch_size={bs}")
            model = HAN(
                max_words=max_sentence_length, max_sentences=max_sentences,
                output_size=len(y_train[0]), embedding_matrix=embedding_matrix
            )

            lr = lr_for_batch(bs)
            optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
            model.compile(loss="categorical_crossentropy", optimizer=optimizer, metrics=['accuracy'])

            if iter_idx == 0 and bs == BATCH_CANDIDATES[-1]:
                model.summary()

            # âœ… ä¿®æ”¹ç‚¹ 1: æ ¹æ®æ€»å¼€å…³ DO_SAVE_MODEL å†³å®šæ˜¯å¦å¯ç”¨ ModelCheckpoint
            es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3, restore_best_weights=True)

            callbacks_list = [es] # EarlyStopping æ€»æ˜¯å¯ç”¨
            if DO_SAVE_MODEL:
                print("[INFO] ModelCheckpoint saving is ENABLED.")
                mc_filepath = os.path.join(tmp_dir, f'iter{iter_idx}_best.weights.h5')
                mc = ModelCheckpoint(
                    filepath=mc_filepath, monitor='val_accuracy', mode='max',
                    verbose=1, save_weights_only=True, save_best_only=True
                )
                callbacks_list.append(mc)
            else:
                print("[INFO] ModelCheckpoint saving is DISABLED.")

            model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=100, #è½®æ¬¡åœ¨è¿™é‡Œ
                batch_size=bs,
                class_weight=class_weights,
                callbacks=callbacks_list, # ä½¿ç”¨æˆ‘ä»¬åŠ¨æ€åˆ›å»ºçš„å›è°ƒåˆ—è¡¨
                verbose=1
            )
            trained, used_bs, used_lr = True, bs, lr
            print(f"[SUCCESS] Training succeeded with batch_size={bs}")
            break

        except tf.errors.ResourceExhaustedError:
            print(f"[WARN] OOM at batch_size={bs}, trying smaller batch size...")
            del model

    if not trained:
        print("[ERROR] All candidate batch sizes failed. Exiting this iteration.")
        return None, {}, (None, None, None, None)

    acc, f1_macro, f1_micro, f1_weighted, _ = _evaluate_on_val(model, X_val, y_val, index_to_label)
    metrics_csv = os.path.join(logs_dir, "iter_metrics.csv")
    header = ["timestamp","iteration","stage","acc","f1_macro","f1_micro","f1_weighted","batch","lr","n_train","n_val"]
    main_metrics_row = {
        "timestamp": datetime.now().isoformat(timespec='seconds'), "iteration": iter_idx,
        "stage": "main_train", "acc": round(acc, 6), "f1_macro": round(f1_macro, 6),
        "f1_micro": round(f1_micro, 6), "f1_weighted": round(f1_weighted, 6),
        "batch": used_bs, "lr": used_lr, "n_train": len(X_train), "n_val": len(X_val)
    }
    append_metrics_csv(metrics_csv, main_metrics_row, header)

    if do_finetune:
        # ... (fine-tuning é€»è¾‘ä¿æŒä¸å˜) ...
        print("\n[FT] Fine-tuning is enabled. Starting fine-tune stage...")
        FINETUNE_BS = 256; FINETUNE_EPOCHS = 3; FINETUNE_LR_FACTOR = 0.3
        try:
            ft_lr = used_lr * FINETUNE_LR_FACTOR if used_lr else 3e-4
            print(f"[FT] Start fine-tuning: bs={FINETUNE_BS}, lr={ft_lr}, epochs={FINETUNE_EPOCHS}")
            es_ft = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2, restore_best_weights=True)
            optimizer_ft = tf.keras.optimizers.Adam(learning_rate=ft_lr)
            model.compile(loss="categorical_crossentropy", optimizer=optimizer_ft, metrics=['accuracy'])
            model.fit(
                X_train, y_train, validation_data=(X_val, y_val),
                epochs=FINETUNE_EPOCHS, batch_size=FINETUNE_BS,
                callbacks=[es_ft], verbose=1, class_weight=class_weights,
            )
            print("[FT] Fine-tune finished.")
            acc_ft, f1_macro_ft, f1_micro_ft, f1_weighted_ft, _ = _evaluate_on_val(model, X_val, y_val, index_to_label)
            append_metrics_csv(metrics_csv, {
                "timestamp": datetime.now().isoformat(timespec='seconds'), "iteration": iter_idx, "stage": "finetune",
                "acc": round(acc_ft,6), "f1_macro": round(f1_macro_ft,6), "f1_micro": round(f1_micro_ft,6),
                "f1_weighted": round(f1_weighted_ft,6), "batch": FINETUNE_BS, "lr": ft_lr,
                "n_train": len(X_train), "n_val": len(X_val)
            }, header)
        except tf.errors.ResourceExhaustedError:
            print("[FT][WARN] OOM during fine-tune, skipping fine-tune step.")
    else:
        print("\n[FT] Fine-tuning is disabled by the global switch. Skipping fine-tune stage.")

    # âœ… ä¿®æ”¹ç‚¹ 2: æ ¹æ®æ€»å¼€å…³ DO_SAVE_MODEL å†³å®šæ˜¯å¦æ‰§è¡Œæœ€ç»ˆçš„æ¨¡å‹ä¿å­˜
    if DO_SAVE_MODEL:
        final_model_path = os.path.join(dump_dir, f"iter{iter_idx}_final.keras")
        print(f"\n[SAVE] Saving final complete model to: {final_model_path}")
        model.save(final_model_path)
    else:
        print("\n[INFO] Final model saving is disabled by the global switch. Skipping.")

    return model, main_metrics_row, (X_train, y_train, X_val, y_val)

def main(dataset_path, print_flag=True, do_finetune=False):
    pkl_dump_dir = dataset_path
    df = pickle.load(open(os.path.join(pkl_dump_dir, "df_contextualized8.pkl"), "rb"))
    word_cluster = pickle.load(open(os.path.join(pkl_dump_dir, "word_cluster_map8.pkl"), "rb"))
    with open(os.path.join(pkl_dump_dir, "seedwords_dritte_cat.json")) as fp:
        label_term_dict = json.load(fp)

    # å¯ç”¨ add_all_interpretationsï¼Œä»¥éµå¾ªåŸä½œè€…çš„å®Œæ•´é€»è¾‘
    label_term_dict = add_all_interpretations(label_term_dict, word_cluster)
    print_label_term_dict(label_term_dict, None, print_components=False)

    labels = list(label_term_dict.keys())
    label_to_index, index_to_label = create_label_index_maps(labels)

    df, word_vec = preprocess(df, word_cluster)
    del word_cluster
    gc.collect()

    word_to_index, index_to_word = create_word_index_maps(word_vec)
    docfreq = calculate_df_doc_freq(df)
    inv_docfreq = calculate_inv_doc_freq(df, docfreq)

    logs_dir = os.path.join(dataset_path, "logs_conwea")
    ensure_dir(logs_dir)

    train_word2vec(df, dataset_path)
    #è¿­ä»£åœ¨è¿™é‡Œ
    num_iterations = 6
    for i in range(num_iterations):
        print(f"\n{'='*35} ITERATION: {i} {'='*35}")
        model, metrics_main, splits = train_classifier(
            iter_idx=i, df=df, labels=labels, label_term_dict=label_term_dict,
            label_to_index=label_to_index, index_to_label=index_to_label,
            dataset_path=dataset_path, logs_dir=logs_dir,
            do_finetune=do_finetune
        )
        if not model:
            print(f"Skipping expand_seeds for iteration {i} due to training failure.")
            continue

        tokenizer = pickle.load(open(os.path.join(dataset_path, "tokenizer8.pkl"), "rb"))
        X_all = prep_data(
            texts=df["sentence"], max_sentences=15,
            max_sentence_length=100, tokenizer=tokenizer
        )
        pred_all = model.predict(X_all, verbose=0)
        pred_labels = get_from_one_hot(pred_all, index_to_label)

        label_term_dict, components = expand_seeds(
            df, label_term_dict, pred_labels, label_to_index, index_to_label,
            word_to_index, index_to_word, inv_docfreq, docfreq, i, n1=5
        )

        seeds_path = os.path.join(logs_dir, f"seedwords_iter{i}.json")
        comps_path = os.path.join(logs_dir, f"components_iter{i}_top20.json")
        save_json(label_term_dict, seeds_path)
        save_json(components_topN(components, N=20), comps_path)

        if print_flag:
            print_label_term_dict(label_term_dict, components)

        # âœ…âœ…âœ… æ–°å¢ï¼šæ‰‹åŠ¨è£å‰ªç§å­è¯çš„äº¤äº’æ¨¡å— âœ…âœ…âœ…
        if INTERACTIVE_PRUNING and i < num_iterations - 1: # åœ¨æœ€åä¸€è½®åæ— éœ€æš‚åœ
            print("\n" + "="*60)
            print(f"â¸ï¸  ITERATION {i} COMPLETE. SCRIPT IS PAUSED.")
            print("  The newly expanded/disambiguated seed words have been saved.")
            print("\n  ACTION REQUIRED:")
            print(f"  1. åœ¨Google Colabå·¦ä¾§çš„æ–‡ä»¶æµè§ˆå™¨ä¸­æ‰¾åˆ°å¹¶æ‰“å¼€æ–‡ä»¶ã€‚")
            print(f"  2. æ–‡ä»¶è·¯å¾„: {seeds_path}")
            print(f"  3. æ‰‹åŠ¨åˆ é™¤æ–‡ä»¶ä¸­ä»»ä½•ä½ ä¸æƒ³è¦çš„ã€æœ‰å™ªå£°çš„ç§å­è¯ã€‚")
            print(f"  4. ä¿å­˜æ–‡ä»¶ (Ctrl+S æˆ–è€… File -> Save)ã€‚")
            print("\n" + "="*60)

            # è¿™è¡Œä»£ç ä¼šæš‚åœè„šæœ¬ï¼Œç›´åˆ°ä½ åœ¨è¿™é‡ŒæŒ‰ä¸‹å›è½¦
            input(">>> åœ¨ä½ ç¼–è¾‘å¹¶ä¿å­˜å®Œæ–‡ä»¶åï¼Œè¯·åœ¨æ­¤å¤„æŒ‰ Enter é”®ç»§ç»­ä¸‹ä¸€è½®è¿­ä»£...")

            # é‡æ–°åŠ è½½ä½ æ‰‹åŠ¨æ¸…ç†è¿‡çš„ç§å­è¯ï¼Œç”¨äºä¸‹ä¸€è½®è¿­ä»£
            print(f"\nğŸ”„ è„šæœ¬å·²æ¢å¤ã€‚æ­£åœ¨ä» {seeds_path} é‡æ–°åŠ è½½æ¸…ç†åçš„ç§å­è¯...")
            with open(seeds_path, "r", encoding="utf-8") as f:
                label_term_dict = json.load(f) # å…³é”®ï¼šç”¨ä½ ä¿®æ”¹è¿‡çš„å†…å®¹è¦†ç›–å†…å­˜ä¸­çš„å˜é‡
            print("âœ… æ¸…ç†åçš„ç§å­è¯å·²åŠ è½½ã€‚å³å°†å¼€å§‹ä¸‹ä¸€è½®è¿­ä»£ã€‚")
        # âœ…âœ…âœ… äº¤äº’æ¨¡å—ç»“æŸ âœ…âœ…âœ…

        if i == num_iterations - 1:
            print("\n--- Generating Final Pseudo-Label Report ---")
            final_results_df = df[['sentence', 'label']].copy()
            final_results_df['final_predicted_label'] = pred_labels
            output_path = os.path.join(logs_dir, f"final_pseudo_labels_report_iter{i}.csv")
            final_results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f" Final report with predicted pseudo-labels saved to: {output_path}")

# ----------------- ä¸»æ‰§è¡ŒåŒº -----------------
if __name__ == '__main__':
    dataset_path = "/content/drive/MyDrive/Dissertation/allinone/ConWea"
    gpu_id = "0"
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)

    print("Checking for GPU devices...")
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print(f" Successfully found {len(gpus)} GPU(s) available.")
        for gpu in gpus:
            print(f"   - Device: {gpu.name}")
    else:
        print(" No GPU devices found. The process will run on CPU.")

    print(f"\nStarting main process with dataset path: {dataset_path}")
    try:
        main(dataset_path=dataset_path, do_finetune=DO_FINETUNE)
        print("\nProcess finished successfully!")
    except Exception as e:
        print(f"\nAn error occurred: {e}")
        import traceback
        traceback.print_exc()
